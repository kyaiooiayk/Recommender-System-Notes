{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What?** WARP = Weighted Approximate Rank Pairwise Loss\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import geom\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.datasets import fetch_movielens\n",
    "# Getting rid of the warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARP\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "- Like the Bayesian Personalized Ranking (BPR) model, **WARP** deals with (user, positive item, negative item) triplets. Unlike BPR, the negative items in the triplet are not chosen by random sampling: they are chosen from among those negative items which would violate the desired item ranking given the state of the model. This approximates a form of active learning where the model selects those triplets that it cannot currently rank correctly.\n",
    "\n",
    "- This procedure yields roughly the following algorithm:\n",
    "\n",
    "    - For a given user, positive item pair, sample a negative item at random from all the remaining items. Compute predictions for both items; if the negative item's prediction exceeds that of the positive item plus a margin, perform a gradient update so we can rank the positive item higher and the negative item lower. If there is no rank violation, continue sampling negative items until a violation is found.\n",
    "    - If we found a violating negative example at the first try, make a large gradient update: this indicates that a lot of negative items are ranked higher than positives items in current state of the model, and the model should be updated by a large amount. If it took a lot of sampling to find a violating example, perform a small update since the model is likely close to the optimum and should be updated at a low rate.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in the dataset\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **LightFM** provides a function for fetching the **MovieLens 100K dataset**\n",
    "- this small recommender dataset, consisting of around 950 users, 1700 movies, and 100,000 ratings. \n",
    "- The ratings are on a scale from 1 to 5, but we'll all treat them as implicit positive feedback in this example. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<943x1682 sparse matrix of type '<class 'numpy.int32'>'\n",
       "\twith 90570 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens = fetch_movielens()\n",
    "train = movielens['train']\n",
    "test = movielens['test']\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPR vs. WARP\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Compares the models' performance between a model that uses two losses: BPR and WARP.\n",
    "- We'll use two ranking metrics to evaluate the performance: precision@k and ROC AUC. \n",
    "- For precision at k we'll be looking at whether they are within the first k = 10 results on the list\n",
    "- For AUC, we'll be calculating the probability that any known positive is higher on the list than a random negative example.\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.189851999282837\n",
      "Precision: train 0.63, test 0.09.\n",
      "AUC: train 0.92, test 0.87.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model = LightFM(learning_rate=0.05, loss='bpr')\n",
    "model.fit(train, epochs=50)\n",
    "print(time() - start)\n",
    "\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.230599880218506\n",
      "Precision: train 0.65, test 0.11.\n",
      "AUC: train 0.95, test 0.91.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "model = LightFM(learning_rate=0.05, loss='warp')\n",
    "model.fit(train, epochs=50)\n",
    "print(time() - start)\n",
    "\n",
    "train_precision = precision_at_k(model, train, k=10).mean()\n",
    "test_precision = precision_at_k(model, test, k=10).mean()\n",
    "\n",
    "train_auc = auc_score(model, train).mean()\n",
    "test_auc = auc_score(model, test).mean()\n",
    "\n",
    "print('Precision: train %.2f, test %.2f.' % (train_precision, test_precision))\n",
    "print('AUC: train %.2f, test %.2f.' % (train_auc, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Despite the fact that WARP optimizes for precision@k its performance, it is able to obtain both a higher recision and ROC AUC score. \n",
    "- However, this is not a **general result**.\n",
    "- **Nevertheless**, in this case we can infer that taking the extra step to ensure the sampled negative item is violating the pairwise comparison and penalizing the loss function more if we were able to find a violating negative item quickly does in fact gives performance boost. \n",
    "\n",
""   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "<hr style = \"border:2px solid black\" ></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- [Youtube: An Introduction to the Geometric Distribution](https://www.youtube.com/watch?v=zq9Oz82iHf0)\n",
    "- [Blog: WARP loss for implicit-feedback recommendation](http://building-babylon.net/2016/03/18/warp-loss-for-implicit-feedback-recommendation/)\n",
    "- [Jupyter Notebook: Learning-to-rank using the WARP loss](http://nbviewer.jupyter.org/github/lyst/lightfm/blob/master/examples/movielens/warp_loss.ipynb)\n",
    "- [Jupyter Notebook: An implicit feedback recommender for the Movielens dataset](http://nbviewer.jupyter.org/github/lyst/lightfm/blob/master/examples/movielens/example.ipynb)\n",
    "- [Paper: J. Weston, S. Bengio, N. Usunier - Wsabie: Scaling up to large vocabulary image annotation (2011)](http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf)\n",
    "- [This notebook](https://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/recsys/5_warp.ipynb)\n",
    "- [lightfm library](https://github.com/lyst/lightfm)\n",
    "- [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/)\n",
    "\n",
""   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
